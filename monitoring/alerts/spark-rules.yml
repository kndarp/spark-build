groups:
- name: Spark_Alerts
  rules:
    - alert: Spark_Dispatcher_Count
      expr: count(spark_mesos_cluster_driver_launched) >= 50
      for: 1m
      labels:
        severity: major
      annotations:
        description: 'Spark has reached desired number of dispatchers'

    - alert: Spark_Dispatcher_Count_Decreasing
      expr: count(spark_mesos_cluster_driver_launched offset 5m) - count(spark_mesos_cluster_driver_launched) > 0
      for: 1m
      labels:
        severity: major
      annotations:
        description: 'Spark dispatcher count unexpectedly decreasing'

    - alert: Spark_Job_Count
      expr: count(spark_mesos_cluster_executor_count) >= 1000
      for: 1m
      labels:
        severity: major
      annotations:
        description: 'Spark has reached desired number of jobs'

    - alert: Spark_Job_Count_Decreasing
      expr: count(spark_mesos_cluster_executor_count offset 5m) - count(spark_mesos_cluster_executor_count) > 0
      for: 1m
      labels:
        severity: major
      annotations:
        description: 'Spark job count unexpectedly decreasing'

    - alert: Spark_CPU_Job
      expr: count(spark_mesos_cluster_executor_count{task_name=~".*monte.*"}) >= 750
      for: 1m
      labels:
        severity: major
      annotations:
        description: 'Spark has reached desired number of CPU jobs'

    - alert: Spark_GPU_Jobs
      expr: count(spark_mesos_cluster_executor_count{task_name=~".*image.*"}) >= 10
      for: 1m
      labels:
        severity: major
      annotations:
        description: 'Spark has reached desired number of GPU jobs'

    - alert: Spark_Streaming_Jobs
      expr: count(spark_mesos_cluster_executor_count{task_name=~".*Kafka.*"}) >= 250
      for: 1m
      labels:
        severity: major
      annotations:
        description: 'Spark has reached desired number of Streaming jobs'
    
    - alert: Spark_Average_Queue_Depth
      expr: avg(spark_mesos_cluster_driver_waiting) >= 50
      for: 3m
      labels:
        severity: major
      annotations:
        description: 'Spark getting backed up'

    - alert: Spark_Average_Job_Duration
      expr: (avg(spark_mesos_cluster_driver_launch_to_finish_state_finished_mean) / 60000) >= 75
      for: 3m
      labels:
        severity: major
      annotations:
        description: 'Spark jobs taking longer to complete than expected'

    - alert: Spark_Average_Launch_Time
      expr: (avg(spark_mesos_cluster_executor_start_to_all_launched_mean) / 1000) >= 30
      for: 3m
      labels:
        severity: major
      annotations:
        description: 'Spark jobs taking longer to launch than expected'

    - alert: Spark_Launch_Time_Stdev
      expr: (stddev(spark_mesos_cluster_executor_start_to_all_launched_mean) / 1000) > 20.0
      for: 3m
      labels:
        severity: major
      annotations:
        description: 'Spark job launch time has high variance'

    - alert: Spark_Job_Success_Rate
      expr: (sum(spark_mesos_cluster_driver_finished_count_mesos_state_task_finished) / (sum(spark_mesos_cluster_driver_finished_count_mesos_state_task_finished) + sum(spark_mesos_cluster_driver_finished_count_mesos_state_task_lost) + sum(spark_mesos_cluster_driver_finished_count_mesos_state_task_failed))) >= 99.0
      for: 1m
      labels:
        severity: major
      annotations:
        description: 'Spark has reached desired success rate'
